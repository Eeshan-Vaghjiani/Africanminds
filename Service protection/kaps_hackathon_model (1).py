# -*- coding: utf-8 -*-
"""kaps hackathon model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vn-J5LkPpGK7lsTQzhbM8qL_ZtkldVXb
"""

pip install pandas scikit-learn scapy

import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score
from sklearn.model_selection import train_test_split
import pandas as pd
import scapy.all as scapy

import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

def load_pcap(file_path):
    packets = scapy.rdpcap(file_path)
    data = []
    for packet in packets:
        ip_src = packet[scapy.IP].src
        ip_dst = packet[scapy.IP].dst
        tcp_srcport = packet[scapy.TCP].sport
        tcp_dstport = packet[scapy.TCP].dport
        ip_proto = packet[scapy.IP].proto
        frame_len = packet.len
        tcp_flags_syn = packet[scapy.TCP].flags.S
        tcp_flags_reset = packet[scapy.TCP].flags.R
        tcp_flags_push = packet[scapy.TCP].flags.P
        tcp_flags_ack = packet[scapy.TCP].flags.A
        tcp_seq = packet[scapy.TCP].seq
        tcp_ack = packet[scapy.TCP].ack
        frame_time = packet.time
        packets_count = len(packets)
        bytes_count = sum(packet.len for packet in packets)
        tx_packets_count = len([p for p in packets if p[scapy.IP].src == ip_src])
        tx_bytes_count = sum([p.len for p in packets if p[scapy.IP].src == ip_src])
        rx_packets_count = len([p for p in packets if p[scapy.IP].dst == ip_dst])
        rx_bytes_count = sum([p.len for p in packets if p[scapy.IP].dst == ip_dst])
        data.append({
            'ip.src': ip_src,
            'ip.dst': ip_dst,
            'tcp.srcport': tcp_srcport,
            'tcp.dstport': tcp_dstport,
            'ip.proto': ip_proto,
            'frame.len': frame_len,
            'tcp.flags.syn': tcp_flags_syn,
            'tcp.flags.reset': tcp_flags_reset,
            'tcp.flags.push': tcp_flags_push,
            'tcp.flags.ack': tcp_flags_ack,
            'tcp.seq': tcp_seq,
            'tcp.ack': tcp_ack,
            'frame.time': frame_time,
            'Packets': packets_count,
            'Bytes': bytes_count,
            'Tx Packets': tx_packets_count,
            'Tx Bytes': tx_bytes_count,
            'Rx Packets': rx_packets_count,
            'Rx Bytes': rx_bytes_count
        })
    df = pd.DataFrame(data)
    return df

def unlabbeled_data(cleaned_df):
    X = cleaned_df[['ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport', 'ip.proto', 'frame.len', 'tcp.flags.syn', 'tcp.flags.reset', 'tcp.flags.push', 'tcp.flags.ack', 'tcp.seq', 'tcp.ack', 'frame.time', 'Packets', 'Bytes', 'Tx Packets', 'Tx Bytes', 'Rx Packets', 'Rx Bytes']]
    Y = cleaned_df['Tx Packets'] > 4  # Assuming a DOS attack is defined as more than 1000 packets from a single source
    return X, Y

def labbeled_data(cleaned_df):
    feature_column = ['ip.src','ip.dst','tcp.srcport','tcp.dstport','ip.proto','tcp.flags.syn','tcp.flags.reset','tcp.flags.push','tcp.flags.ack','ip.flags.mf','ip.flags.df','ip.flags.rb','tcp.seq','tcp.ack','frame.time','Packets','Bytes','Tx Packets','Tx Bytes','Rx Packets','Rx Bytes']
    X = cleaned_df[feature_column]
    Y = cleaned_df['Label']
    return X, Y

def detection_rate_score(cm):
    return cm[0][0]/(cm[0][0]+cm[1][0])

def false_positive_score(cm):
    return cm[0][1]/(cm[0][1]+cm[1][1])

def metrics_print(model_name, Y_test, Y_pred):
    conf_matrix = confusion_matrix(Y_test,Y_pred)
    print(model_name,'\n',
          'accuracy score: ',accuracy_score(Y_test,Y_pred),'\n',
          'precision score: ',precision_score(Y_test,Y_pred),'\n',
          'detection rate score: ',detection_rate_score(conf_matrix),'\n',
          'false positive score: ',false_positive_score(conf_matrix))

    # Create a heatmap using Seaborn
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="rocket", cbar=False,
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])

    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

def train_isolation_forest(X_train, X_test):
    # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.85, random_state=42)
    iforest = IsolationForest(n_estimators=140, contamination=0.21)
    iforest.fit(X_train)
    Y_pred = iforest.predict(X_test)
    return iforest, Y_pred

def train_svm(X_train, X_test, Y_train, Y_test):
    # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75, random_state=42)
    svm_model = svm.SVC(kernel='poly', C=1)
    # svm_model = svm.SVC(kernel='rbf', C=1)
    svm_model.fit(X_train, Y_train)
    Y_pred = svm_model.predict(X_test)
    metrics_print('SVM', Y_test, Y_pred)
    return svm_model, Y_pred

def predict(iforest, svm_model, unlabelled_X, labelled_X):
    Y_pred_iforest = iforest.predict(unlabelled_X)
    Y_pred_svm = svm_model.predict(labelled_X)
    Y_pred_combined = np.logical_or(Y_pred_iforest == -1, Y_pred_svm == 1).astype(int)
    return Y_pred_iforest, Y_pred_svm, Y_pred_combined

def predict_and_combine(iforest, svm_model, labelled_X_test, labelled_Y_test, unlabelled_X_test, unlabelled_Y_test):
    Y_pred_iforest, Y_pred_svm, Y_pred_combined = predict(iforest, svm_model, unlabelled_X_test, labelled_X_test)
    accuracy_iforest = accuracy_score(unlabelled_Y_test, Y_pred_iforest)
    accuracy_svm = accuracy_score(labelled_Y_test, Y_pred_svm)
    accuracy_combined = accuracy_score(labelled_Y_test, Y_pred_combined)
    return Y_pred_combined, accuracy_iforest, accuracy_svm, accuracy_combined

from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

files.upload()

df = pd.read_csv('/content/drive/MyDrive/KAPS_HACKATHON_MODEL/APA-DDoS-Dataset.csv', delimiter=',')
print(df.head())

df['Label'] = df['Label'].apply(lambda val: 1 if val in ['DDoS-PSH-ACK', 'DDoS-ACK'] else 0)
df.head()

def normalize_data(df):
    columns = df.columns.values
    for column in columns:
        text_digit_vals = {}
        def convert_to_int(val):
            return text_digit_vals[val]

        if df[column].dtype != np.int64 and df[column].dtype != np.float64:
            column_contents = df[column].values.tolist()
            unique_elements = set(column_contents)
            x = 0
            for unique in unique_elements:
                if unique not in text_digit_vals:
                    text_digit_vals[unique] = x
                    x+=1

            df[column] = list(map(convert_to_int, df[column]))

    return df

cleaned_df = normalize_data(df)
cleaned_df.head()

sns.histplot(data=df, x='Label', hue='Label', multiple='stack')
plt.show()

def train_model():
    cleaned_df_train, cleaned_df_test = train_test_split(cleaned_df, train_size=0.85, random_state=50)


    labelled_X_train, labelled_Y_train = labbeled_data(cleaned_df_train)
    labelled_X_test, labelled_Y_test = labbeled_data(cleaned_df_test)

    unlabelled_X_train, unlabelled_Y_train = unlabbeled_data(cleaned_df_train)
    unlabelled_X_test, unlabelled_Y_test = unlabbeled_data(cleaned_df_test)

    svm_model, Y_pred_svm = train_svm(labelled_X_train, labelled_X_test, labelled_Y_train, labelled_Y_test)

    iforest, Y_pred_iforest = train_isolation_forest(unlabelled_X_train, unlabelled_X_test)

    Y_pred_combined, accuracy_iforest, accuracy_svm, accuracy_combined = predict_and_combine(iforest, svm_model, labelled_X_test, labelled_Y_test, unlabelled_X_test, unlabelled_Y_test)
    print('Accuracy of Isolation Forest:', accuracy_iforest)
    print('Accuracy of SVM:', accuracy_svm)
    print('Accuracy of combined model:', accuracy_combined)
    return iforest, svm_model

iforest, svm_model = train_model()

def pred(data):
    cleaned_data = normalize_data(data)
    dataX, dataY = unlabbeled_data(cleaned_data)
    predict(iforest, svm_model, dataX, dataY)

data = files.upload()

def predict_from_file(file_path, iforest, svm_model):
    """Predicts the values for a given file using the trained Isolation Forest and SVM models.

    Args:
        file_path (str): The path to the input file (csv or pcap).
        iforest: The trained Isolation Forest model.
        svm_model: The trained SVM model.

    Returns:
        numpy.ndarray: The predicted values (0 or 1).
    """
    if file_path.endswith('.csv'):
        data = pd.read_csv(file_path, delimiter=',')
    else:
        data = load_pcap(file_path)  # Use your load_pcap function

    cleaned_data = normalize_data(data)
    unlabelled_X, _ = unlabbeled_data(cleaned_data)
    labelled_X, _ = labbeled_data(cleaned_data)

    Y_pred_iforest, Y_pred_svm, Y_pred_combined = predict(iforest, svm_model, unlabelled_X, labelled_X)

    # You can return any of the predictions (iforest, svm, combined)
    # Here, I'm returning the combined prediction
    return [Y_pred_iforest, Y_pred_svm, Y_pred_combined]

predictions = predict_from_file('APA-DDoS-Dataset (1).csv', iforest, svm_model)
print(predictions[0]) 
print(predictions[1]) 
print(predictions[2]) 
sum = predictions[2] / len(predictions[2])
print('Threat detected' if sum > 0.2 else 'No threatÂ detected')

# for name, data in data.items():
#     with open(name, 'wb') as f:
#         f.write(data)

#     if name.endswith('csv'):
#         data = pd.read_csv(name, delimiter=',')
#     else:
#         # data = load_pcap(data)
#     pred(data)
#     print ('saved file', name)

# def process_file(name, content):
#     if name.endswith('.csv'):
#         data = pd.read_csv(name, delimiter=',')
#     else:
#         # For pcap files, write the bytes to a temporary file first
#         with open(name, 'wb') as f:
#             f.write(content)
#         data = load_pcap(name)
#     pred(data)
#     print('processed file', name)

# # Modified upload handling
# uploaded = files.upload()
# if isinstance(uploaded, dict):
#     # Multiple files
#     for name, content in uploaded.items():
#         process_file(name, content)
# else:
#     # Single file
#     name = list(uploaded.keys())[0]  # Get the filename
#     content = uploaded[name]
#     process_file(name, content)